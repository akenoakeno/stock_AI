{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf2dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim import Adam\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2aad0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkDataSet(data_size, data_length=50, freq=60.):\n",
    "    train_x = []\n",
    "    train_t = []\n",
    "\n",
    "    for offset in range(data_size):\n",
    "        train_x.append([[math.sin(2*math.pi*(offset+i)/freq)+np.random.normal(loc=0.0, scale=0.05)] for i in range(data_length)])\n",
    "        train_t.append([math.sin(2*math.pi*(offset+data_length)/freq)])\n",
    "\n",
    "    return train_x, train_t #train_x=(data_size, data_length, 1), train_t=(data_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1ccc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.kernel_size = 5\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=inputDim, hidden_size=hiddenDim, batch_first=True) #batch_first=Trueで(seq, batch, vec)->(batch, seq, vec)に入力の形を変更\n",
    "        self.conv1d = nn.Conv1d(1, 1, self.kernel_size)\n",
    "        self.output_layer = nn.Linear(hiddenDim - self.kernel_size + 1, outputDim)\n",
    "\n",
    "    def forward(self, inputs, hidden0=None):\n",
    "        output, (hidden, cell) = self.rnn(inputs, hidden0) #LSTMのforwardのreturnはこのような戻り値になっている\n",
    "        output = self.conv1d(output[:, -1, :])\n",
    "        output = self.output_layer(output) #LSTMのoutput=(batch, seq, hidden)からseqのみ最後のやつだけを取り出す\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3152d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "fn_prefix = \"sin_test_%s\" % datestr\n",
    "summary_writer = tensorboard.SummaryWriter(\"runs/%s\" % fn_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44a1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 100\n",
    "epoch_num = 100\n",
    "hidden_size = 10\n",
    "\n",
    "train_x, train_t = mkDataSet(training_size)\n",
    "\n",
    "model = Predictor(1, hidden_size, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = RMSprop(model.parameters(), lr=0.01)\n",
    "summary_writer.add_graph(model, torch.tensor([train_x[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66131101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 3.896690, training_accuracy: 0.58000\n",
      "2 loss: 9.581342, training_accuracy: 0.29000\n",
      "3 loss: 8.099065, training_accuracy: 0.27000\n",
      "4 loss: 4.770816, training_accuracy: 0.34000\n",
      "5 loss: 2.921432, training_accuracy: 0.45000\n",
      "6 loss: 2.080489, training_accuracy: 0.69000\n",
      "7 loss: 0.884238, training_accuracy: 0.89000\n",
      "8 loss: 0.552981, training_accuracy: 0.90000\n",
      "9 loss: 0.726996, training_accuracy: 0.85000\n",
      "10 loss: 0.828510, training_accuracy: 0.83000\n",
      "11 loss: 0.858157, training_accuracy: 0.82000\n",
      "12 loss: 0.700398, training_accuracy: 0.88000\n",
      "13 loss: 0.557784, training_accuracy: 0.85000\n",
      "14 loss: 0.412043, training_accuracy: 0.89000\n",
      "15 loss: 0.535763, training_accuracy: 0.89000\n",
      "16 loss: 0.447848, training_accuracy: 0.92000\n",
      "17 loss: 0.646237, training_accuracy: 0.79000\n",
      "18 loss: 0.509941, training_accuracy: 0.89000\n",
      "19 loss: 0.533003, training_accuracy: 0.91000\n",
      "20 loss: 0.448990, training_accuracy: 0.89000\n",
      "21 loss: 0.488933, training_accuracy: 0.87000\n",
      "22 loss: 0.497936, training_accuracy: 0.87000\n",
      "23 loss: 0.439440, training_accuracy: 0.89000\n",
      "24 loss: 0.523165, training_accuracy: 0.88000\n",
      "25 loss: 0.464930, training_accuracy: 0.93000\n",
      "26 loss: 0.510013, training_accuracy: 0.89000\n",
      "27 loss: 0.399725, training_accuracy: 0.92000\n",
      "28 loss: 0.386317, training_accuracy: 0.90000\n",
      "29 loss: 0.468322, training_accuracy: 0.88000\n",
      "30 loss: 0.458700, training_accuracy: 0.91000\n",
      "31 loss: 0.325246, training_accuracy: 0.92000\n",
      "32 loss: 0.376131, training_accuracy: 0.92000\n",
      "33 loss: 0.396021, training_accuracy: 0.94000\n",
      "34 loss: 0.399093, training_accuracy: 0.89000\n",
      "35 loss: 0.431150, training_accuracy: 0.86000\n",
      "36 loss: 0.425770, training_accuracy: 0.91000\n",
      "37 loss: 0.381672, training_accuracy: 0.92000\n",
      "38 loss: 0.350662, training_accuracy: 0.91000\n",
      "39 loss: 0.345546, training_accuracy: 0.90000\n",
      "40 loss: 0.397132, training_accuracy: 0.87000\n",
      "41 loss: 0.406555, training_accuracy: 0.89000\n",
      "42 loss: 0.337377, training_accuracy: 0.95000\n",
      "43 loss: 0.379096, training_accuracy: 0.91000\n",
      "44 loss: 0.341247, training_accuracy: 0.92000\n",
      "45 loss: 0.321252, training_accuracy: 0.91000\n",
      "46 loss: 0.356849, training_accuracy: 0.90000\n",
      "47 loss: 0.336070, training_accuracy: 0.91000\n",
      "48 loss: 0.410266, training_accuracy: 0.87000\n",
      "49 loss: 0.437016, training_accuracy: 0.85000\n",
      "50 loss: 0.299161, training_accuracy: 0.93000\n",
      "51 loss: 0.367232, training_accuracy: 0.91000\n",
      "52 loss: 0.396329, training_accuracy: 0.89000\n",
      "53 loss: 0.424641, training_accuracy: 0.90000\n",
      "54 loss: 0.377904, training_accuracy: 0.89000\n",
      "55 loss: 0.277574, training_accuracy: 0.93000\n",
      "56 loss: 0.359139, training_accuracy: 0.92000\n",
      "57 loss: 0.393332, training_accuracy: 0.86000\n",
      "58 loss: 0.295352, training_accuracy: 0.93000\n",
      "59 loss: 0.338960, training_accuracy: 0.89000\n",
      "60 loss: 0.362418, training_accuracy: 0.89000\n",
      "61 loss: 0.316343, training_accuracy: 0.93000\n",
      "62 loss: 0.362314, training_accuracy: 0.92000\n",
      "63 loss: 0.342804, training_accuracy: 0.94000\n",
      "64 loss: 0.342000, training_accuracy: 0.90000\n",
      "65 loss: 0.334140, training_accuracy: 0.92000\n",
      "66 loss: 0.366955, training_accuracy: 0.94000\n",
      "67 loss: 0.382203, training_accuracy: 0.87000\n",
      "68 loss: 0.317662, training_accuracy: 0.93000\n",
      "69 loss: 0.313644, training_accuracy: 0.93000\n",
      "70 loss: 0.319145, training_accuracy: 0.92000\n",
      "71 loss: 0.374538, training_accuracy: 0.90000\n",
      "72 loss: 0.329353, training_accuracy: 0.96000\n",
      "73 loss: 0.373636, training_accuracy: 0.90000\n",
      "74 loss: 0.324875, training_accuracy: 0.89000\n",
      "75 loss: 0.306287, training_accuracy: 0.92000\n",
      "76 loss: 0.325143, training_accuracy: 0.92000\n",
      "77 loss: 0.401837, training_accuracy: 0.88000\n",
      "78 loss: 0.304592, training_accuracy: 0.93000\n",
      "79 loss: 0.276296, training_accuracy: 0.95000\n",
      "80 loss: 0.289929, training_accuracy: 0.97000\n",
      "81 loss: 0.289446, training_accuracy: 0.95000\n",
      "82 loss: 0.319994, training_accuracy: 0.91000\n",
      "83 loss: 0.440857, training_accuracy: 0.89000\n",
      "84 loss: 0.372900, training_accuracy: 0.90000\n",
      "85 loss: 0.361813, training_accuracy: 0.92000\n",
      "86 loss: 0.312751, training_accuracy: 0.94000\n",
      "87 loss: 0.298014, training_accuracy: 0.92000\n",
      "88 loss: 0.327193, training_accuracy: 0.94000\n",
      "89 loss: 0.294725, training_accuracy: 0.94000\n",
      "90 loss: 0.333886, training_accuracy: 0.91000\n",
      "91 loss: 0.307817, training_accuracy: 0.94000\n",
      "92 loss: 0.360079, training_accuracy: 0.88000\n",
      "93 loss: 0.273256, training_accuracy: 0.95000\n",
      "94 loss: 0.276493, training_accuracy: 0.95000\n",
      "95 loss: 0.318956, training_accuracy: 0.90000\n",
      "96 loss: 0.382520, training_accuracy: 0.91000\n",
      "97 loss: 0.300862, training_accuracy: 0.93000\n",
      "98 loss: 0.373312, training_accuracy: 0.90000\n",
      "99 loss: 0.304378, training_accuracy: 0.90000\n",
      "100 loss: 0.253513, training_accuracy: 0.98000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    last_training_accuracy = 0.0\n",
    "    correct = 0.0\n",
    "    train_x, train_t = mkDataSet(training_size)\n",
    "    for i in range(training_size):\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.tensor([train_x[i]])\n",
    "        label = torch.tensor([train_t[i]])\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data\n",
    "        correct += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
    "    \n",
    "    training_accuracy = correct / training_size\n",
    "    print('%d loss: %3f, training_accuracy: %.5f' % (epoch+1, running_loss, training_accuracy))\n",
    "    if last_training_accuracy > training_accuracy:\n",
    "        break\n",
    "    last_training_accuracy = training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0779b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkRandomBatch(train_x, train_t, batch_size=10):\n",
    "    batch_x = []\n",
    "    batch_t = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        idx = np.random.randint(0, len(train_x)-1)\n",
    "        batch_x.append(train_x[idx])\n",
    "        batch_t.append(train_t[idx])\n",
    "    \n",
    "    return torch.tensor(batch_x), torch.tensor(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75ade32",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 1, 5], expected input[1, 10, 10] to have 1 channels, but got 10 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m data, label \u001b[38;5;241m=\u001b[39m mkRandomBatch(train_x, train_t, batch_size)\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPredictor.forward\u001b[0;34m(self, inputs, hidden0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, hidden0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(inputs, hidden0) \u001b[38;5;66;03m#LSTMのforwardのreturnはこのような戻り値になっている\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(output) \u001b[38;5;66;03m#LSTMのoutput=(batch, seq, hidden)からseqのみ最後のやつだけを取り出す\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    296\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    297\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 1, 5], expected input[1, 10, 10] to have 1 channels, but got 10 channels instead"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    last_training_accuracy = 0.0\n",
    "    correct = 0.0\n",
    "    for i in range(int(training_size / batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        data, label = mkRandomBatch(train_x, train_t, batch_size)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data\n",
    "        correct += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
    "    \n",
    "    training_accuracy = correct / training_size\n",
    "    print('%d loss: %3f, training_accuracy: %.5f' % (epoch+1, running_loss, training_accuracy))\n",
    "    if last_training_accuracy > training_accuracy:\n",
    "        break\n",
    "    last_training_accuracy = training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd246a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
